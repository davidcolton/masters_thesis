\section{Common Obstacles to Overcome}
\label{section:3.3}

It was mentioned at the end of the previous section that a number of issues or obstacles reappeared again and again throughout the literature review. This next section will give a brief overview of these issues and, where offered, some of the suggested solutions. The topics include anonymity, the availability of suitable datasets, classification or labelling, and class imbalance.

\subsection{Anonymity}

\citet{dinakar_modeling_2011} identifies that the ability to anonymously post hurtful content online that targets another person and the absence of meaningful supervision of the electronic medium as two of the factors contributing to the increasing social menace of cyberbullying. \citet{yin_detection_2009} suggests that the anonymity offered in on-line communities gives the bully the perception of a ``safer'' environment where their real identity is hidden implying that they are free to bully without fear of discovery. \citet{nahar_effective_2013} highlights that an anonymous cyberbullying post can create a heightened sense of fear in the victim as they are confronted with the unknown. The anonymity gives the bully a feeling of power and control over their victim. \citet{reynolds_using_2011} and \citet{kontostathis_detecting_2013} both agree that www.formstring.me, a questions and answers format social media site, which permits the posting of an anonymous question to a user of the site, is prone to cyberbullying because of this offer of anonymity. One interesting solution to the anonymous posting of content online, the Real Name Verification Law, is described in \citet{cho_empirical_2012}. This law, introduced in South Korea in 2007, meant that before users could post on popular sites they first had to verify their real name. The Real Name Verification Law was overturned in 2012 \cite{sang-hun_south_2012}.

\subsection{Data and Classification}

Nearly every major paper reviewed expressed frustration at the lack of a standard labelled dataset that could be used in the research of cyberbullying detection. \citet{yin_detection_2009} and \citet{nahar_effective_2013} both used the MySpace, Kongragate and Slashdot datasets from Fundaci√≥n Barcelona Media datasets provided for the CAW 2.0 Workshop \cite{fundacion_barcelona_media_fbm_caw_2009}. \citet{nahar_effective_2013} also used the manually labelled dataset from \citet{yin_detection_2009} as their ground truth. \citet{dinakar_modeling_2011} used comments from YouTube videos as their data, treating each comment as a stand-alone comment. \citet{dadvar_improving_2013} also used YouTube video comments but \citet{xu_fast_2012} and \citet{xu_learning_2012} used Twitter tweets as their dataset.  \citet{kontostathis_detecting_2013} and \citet{reynolds_using_2011} both used data that was scraped from the www.formspring.me website.

Once the data had been acquired the next issue was the classification or labelling of the data to identify those posts which were considered to contain cyberbullying content and posts that did not. In most cases, the data was manually classified using a simple binary label \cite{dadvar_towards_2012} \cite{dadvar_improving_2013}  \cite{dinakar_modeling_2011} \cite{yin_detection_2009} by annotators known to the authors but the Mechanical Turk service offered by Amazon was also used on occasion  \cite{kontostathis_detecting_2013} \cite{reynolds_using_2011}. \citet{xu_fast_2012} and \citet{xu_learning_2012} used Twitter tweets for their dataset and rather than classifying by hand an enriched dataset was created by automatically filtering on tweets that contained the ``bully'', ``bullying'' and ``bullied'' keywords. In \citet{chen_detecting_2012} a semi-automated process was used where words were automatically identified as profane using a dictionary and weighted according to their offensiveness.

\subsection{Class Imbalance}

It was also recognised that there existed a class imbalance between the positive bullying class, and the negative class \cite{dadvar_towards_2012} \cite{chen_detecting_2012}. \citet{reynolds_using_2011} found that only 7.2\% of their training dataset was given a positive classification. As a result of this imbalance, it was observed that the chosen learner could achieve accuracy figures of over 90\% by ignoring the cyberbullying cases and by just labelling everything as not containing cyberbullying. \citet{yin_detection_2009} and \citet{reynolds_using_2011} address this issue by replicating the positive classes.

\citet{weiss_cost-sensitive_2007} suggests that most classifiers are designed to maximise accuracy implying that when used to label a highly imbalanced dataset the more frequently occurring class will be predicted. However, when labelling such a highly skewed dataset it is usually the case that it is the less frequently occurring class that is of interest, for example in medical diagnostics and fraud detection. Three methods were evaluated to handle class imbalance. The first method uses  Randon Under Sampling (RUS) or the majority class. The second uses Random Over Sampling (ROS) of the minority class. The third method is a cost based method where the cost of misclassification is built into the learner. Under and over sampling are not without their disadvantages. Under sampling runs the risk of discarding potentially useful data whilst over sampling, besides increasing the learning time required, can lead to over fitting or the generation of a rule specifically for the replicated data. In their experiments a number of datasets were used with the C5.0 cost sensitive decision tree learner \cite{rulequest_research_data_2013},  an enhanced version of C4.5 \cite{quinlan_c4.5:_1993} learner. For the experiments, the unbalanced datasets were submitted to the C5.0 learner with a variety of costs for misclassification. Each dataset was then rebalanced using both over and under sampling to replicate a ratio that mimicked the cost of misclassification and submitted to the C5.0 learner without a cost. Though the overall result was inconclusive, by measuring the total cost of misclassification at the various costs / ratios it was found that for larger datasets, greater than 10,000 examples, the cost sensitive algorithm outperformed both over and under sampling methods. Over sampling was found to perform the best on smaller datasets, in the experiments this was on datasets of less than 250 examples.

\citet{kubat_addressing_1997} refers to the problem when one dataset is significantly larger than the other as ``Addressing the Curse of Imbalanced Training Sets''. They identify that it is usually the minority class that is of interest and point out that a classifier that achieves 99.8\% accuracy where the minority positive class only consists of 0.2\% of the samples is, in fact, of no use at all if it the presence of the samples of interest are completely ignored. In addition to standard classifier performance measures, for example accuracy, precision, recall and F-Measure \citeauthor{kubat_addressing_1997} describe another measure that uses the geometric mean of the accuracies measured separately on each class as $g =\sqrt{\frac{TP}{TP + FN} \cdot \frac{TN}{TN + FP}}$. The goal of this measure is to maximise the accuracies of both classes but at the same time keeping them balanced such that a poor value for either the positive, or negative class, will give an overall poor performance for the classifier.

There are other papers that suggest alternative approaches to tackle class imbalances. \citet{chawla_smote:_2002} uses a combination of under sampling of the majority class and over sampling of the minority class using their novel algorithm SMOTE, Synthetic Minority Over-sampling Technique, to create synthetic minority class examples.\citet{cardie_improving_1997} presents two case based learning frameworks in a natural language environment that uses information gained from analysing a baseline case to determine the appropriate class weighting. In \citet{chan_toward_1998} the dollar value cost of fraudulent credit card transactions are incorporated into the model as a cost for making an incorrect prediction. A multi classifier meta learning approach is used to handle the non uniform distribution of the samples. \citet{Garcia:2007} provide an in-depth review of the important research in the area of class imbalance in pattern learning and classification.
 
 
 
 
 
 
 