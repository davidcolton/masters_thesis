\section{Summary and Conclusion}
\label{section:sec5:summary_conclusion}

This chapter focused on describing how Python, NLTK and Scikit-Learn were used in the development of a number of models that could be used to classify samples as either bullying or not bullying. The modelling processes used were broken into several sections where each section built on the ideas introduced, and results achieved, in the previous sections cumulating in Section \ref{subsection:applying-classifiers} where the five best models developed were analysed in detail.

In \ref{subsection:nltk-initial_modelling} the modelling techniques and required corpus structure were introduced. The datasets generated in Chapter \ref{chapter4} were read in and the features, or tokens, present in each of the corpora were extracted before being used to train a Naive Bayes classifier. Although the performance of these first classifiers were not great, especially on the positive bullying class, the necessary first steps to develop a better model had been successfully navigated. It was also examined whether the n-gram generation built into the NLTK performed any better than the pre-generated datasets. It was found that there was no performance improvement in the model and that generation took longer. 

In Section \ref{subsection:nltk-class_imbalance} the issue of class imbalance was addressed. The negative majority class, in this case not bullying samples, significantly outnumbered the positive bullying class. To address this under sampling of the majority class, over sampling of the minority class and hybrid sampling approaches were tested using the same model and datasets developed in the previous section. Although some of the approaches gave very promising results, it was highlighted that over sampling too much could lead to over fitting and under sampling can lead to data loss. In this section there was also an investigation into balancing the classes using the most frequently occurring features but this path did not yield results worth pursuing.

In Section \ref{subsection:scikit-learn} the Scikit-Learn Python package was introduced, and an initial suite of models were developed. Rather than using feature based learning, introduced in the development of the NLTK models, the Scikit-Learn models instead used a Term Frequency, Inverse Document Frequency  (TF-IDF) approach. In addition to using TF-IDF, only one dataset was used as it was found that the n-gram generation included with Scikit-Learn gave better results then the manually generated n-gram datasets. A Naive Bayes learner was again used, but a Linear Support Vector Classifier was introduced. Although there was no significant performance improvement using Scikit-Learn there was a noted decrease in execution time for model generation and samples classification.

In Section \ref{subsection:scikit-class-imbalance}, in addition to tackling class imbalance using the same sampling techniques described in Section \ref{subsection:nltk-class_imbalance}, the Pipeline and Grid Search packages were introduced that allowed an automated approach to determining the best parameter values for the TF-IDF, Naive Bayes and Linear SV operators. Similar promising results to those achieved using sampling and the NLTK were achieved but again the faster total run time of the Scikit-Learn models were highlighted.

Cost sensitive Learning using Scikit-Learn was attempted in Section 
\ref{subsection:scikit-learn-cost} but the results were either disappointing or the models proved difficult to train either predicting all samples as positive or negative. 

The criteria to be used to determine the best models was described in Section \ref{subsection:best-classifiers}. The g-performance value of every model developed, $g =\sqrt{\frac{TP}{TP + FN} \cdot \frac{TN}{TN + FP}}$, was calculated and a heat map of the results was used to identify the top 24 models. In these 24 models there were eight NLTK over sampling models, eight NLTK hybrid sampling models and eight assorted scikit-learn models.

Section \ref{subsection:applying-classifiers} began by providing a description of the scenario to be used to evaluate the models identified in Section \ref{subsection:best-classifiers}. The simulation and hold back datasets were introduced, and preprocessing required discussed. Each of the 24 models then classified the hold back dataset and the five best candidates were identified. In the top models there were three scikit-learn models and two NLTK models. These top models were further analysed by simulating the evolution of each as new, unseen samples, were classified and then fed back into the training data for the next iteration. A control dataset was labelled as the hold back dataset and the second dataset, the simulation dataset, was used to evolve the models. The NLTK models were quickly discarded as both were found to be over predicting samples as bullying. Following further analysis of the Scikit-Learn models it was determined that the Linear Support Vector classifier using 1:1 over sampling, stop words included, uni-grams and bi-grams was deemed the best solution developed.