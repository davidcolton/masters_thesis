\section{Precision, Recall, Accuracy and G-Performance}
\label{section:2.3}

When evaluating the performance of a model in predicting whether a question was correctly classified as bullying or not precision, recall and accuracy will be used. When predicting whether a question is bullying there are four possible outcomes:

\begin{enumerate}

	\item \textbf{True Positive} (TP)\\
	A true positive is where the question is predicted as bullying and was also classified as bullying
	\item \textbf{False Positive} (FP) \\
	A false positive is where the question is predicted as bullying but was also classified as not bullying
	\item \textbf{False Negative} (FN)\\
	A false negative is where the question is predicted as not bullying but was also classified as bullying
	\item \textbf{True Negative} (TN)\\
	A true negative is where the question is predicted as not bullying and was also classified as not bullying

\end{enumerate}

The overall accuracy of a model is calculated as:
\begin{equation}
     \frac{Number of True Positives + Number of True Negatives}{Total Number of Examples}
\end{equation}

Positive Class Precision is calculated as:
\begin{equation}
     \frac{Number of True Positives}{Number of True Positives + Number of False Positives}
\end{equation}

Positive Class Recall is calculated as:
\begin{equation}
     \frac{Number of True Positives}{Number of True Positives + Number of False Negatives}
\end{equation} 

Negative Class Precision is calculated as:
\begin{equation}
     \frac{Number of True Negatives}{Number of True Negatives + Number of False Negatives}
\end{equation}

Negative Class Recall is calculated as:
\begin{equation}
     \frac{Number of True Negatives}{Number of True Negatives + Number of False Positives}
\end{equation} 


Precision and recall are inversely related meaning that as precision increases recall decreases and inversely where recall increases precision decreases. When developing a classification model the critical decision is whether to seek to have high precision and low recall or to develop a model that delivers a low precision value but has high recall. Consider a scenario where we are trying to classify questions as bullying. High precision and low recall values suggest that a high percentage of questions predicted as bullying will be bullying. However, a significant number of bullying questions will not be correctly identified. A high recall value implies that a large percentage of bullying questions have been correctly identified but, as a consequence, a large number of not bullying questions would also be incorrectly identified as bullying yielding low precision. Usually a trade-off has to be made between precision and recall depending on the situation and the preferred outcomes.

\subsection{G-Performance}
In addition to standard classifier performance measures, for example accuracy, precision, recall and F-Measure \citeauthor{kubat_addressing_1997} \cite{kubat_addressing_1997} describes another measure that uses the geometric mean of the accuracies measured separately on each class called the g-performance. The goal of this measure is to maximise the recall of both class but at the same time keeping them balanced such that a poor value for either the positive or negative class will give an overall poor performance for the classifier.

G-Performance is calculated as:
\begin{equation}
     g =\sqrt{\frac{True Positives}{True Positives + False Negatives} \cdot \frac{True Negatives}{True Negatives + False Positives}}
\end{equation} 